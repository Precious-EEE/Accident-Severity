# -*- coding: utf-8 -*-
"""Accident_severity_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tPmEzq8GtGoLehKFl8F75HkVNucXwLNp

#  IMPORTING OF LIBRARIES
"""

# Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from math import pi
import seaborn as sns
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris

from sklearn.dummy import DummyClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from keras.models import Sequential
from keras.layers import Dense, Dropout

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

from google.colab import drive
drive.mount('/content/drive')

"""# LOADING THE DATA"""

#Load files
train = pd.read_csv('OPTION1_uk_road_accident_2019_coursework_final.csv')

#preveiw the dataset
train.head()

"""# INFORMATION ABOUT THE DATASET"""

#dataset info
train.info()

#shape of the dataset
train.shape

#column of the dataset
train.columns

#column types
train.dtypes

"""#STATISTICAL SUMMARY"""

#train statistical summary
train.describe()

"""#MISSING VALUES"""

#value of nan in each column
train.isna().sum()

train["age_of_oldest_driver"]

train["accident_severity"]

#plot of nan values
train.isna().sum().plot(kind="bar")
plt.show()

"""#FILLING THE MISSING VALUES"""

# most common value of accident_severity
train["accident_severity"].mode()

# filling nan with the common value
train["accident_severity"].fillna(train["accident_severity"].mode()[0],inplace=True)
train["accident_severity"].isna().values.any()

# mean of age_of_oldest_driver
train["age_of_oldest_driver"].mean()

#median of age_of_oldest_driver
train["age_of_oldest_driver"].median()

# filling nan with the median
train["age_of_oldest_driver"].fillna(train["age_of_oldest_driver"].median(),inplace=True)
train["age_of_oldest_driver"].isna().values.any()

#sum of the nan in the dataset
train.isna().sum()

"""#EXPLORATORY DATA ANALYSIS"""

# duplicated value
train.duplicated().sum()

#drop dupllicate
train.drop_duplicates(inplace=True)

train.head()

#unique values
train['light_conditions'].unique()

train['weather_conditions'].unique()

train['road_surface_conditions'].unique()

train['vehicle_type'].unique()

# drop index
train = train.drop(["accident_index", ], axis=1)

# list of object
categorical_columns = train.select_dtypes('object').columns.to_list()
categorical_columns

# Plot of object columns in relation with the target column
for i in categorical_columns:
    plt.figure(figsize=(15,10))
    sns.countplot(x=i,hue='accident_severity',data=train,palette='ocean')
    plt.xlabel(i,fontsize=10)

#relationship between the numerical column
plt.figure(figsize=(14,12))

sns.scatterplot(data=train,x="age_of_oldest_driver",y="speed_limit",s=70,hue="accident_severity",palette='ocean')

plt.show()

#boxplot of the numerical columns
train[["age_of_oldest_driver"]].boxplot()

train[["speed_limit"]].boxplot()

#correlation
train.corr()

#correlation plot
fig,ax = plt.subplots(figsize=(9,7))
correlations = train.corr()
sns.heatmap(correlations,cmap="YlGnBu",annot=True)
plt.show()

# turn capital letter strings to small
train["accident_severity"] = train["accident_severity"].str.lower()

# the values in the column
train['accident_severity'].unique()

"""CATEGORIZING / ENCODING FEATURES"""

# categorizing string values in the columns
cat_features = ['light_conditions',
 'weather_conditions',
 'road_surface_conditions',
 'vehicle_type',
 'junction_location',
 'skidding_and_overturning',
 'vehicle_leaving_carriageway',
 'hit_object_off_carriageway',
 'first_point_of_impact',
 'sex_of_driver','accident_severity']
for cat_feature in cat_features:
    train[f"{cat_feature}_cat"] = train[cat_feature].astype('category')
    train[f"{cat_feature}_cat"] = train[f"{cat_feature}_cat"].cat.codes

train.head()

#dropping string columns after replacing with another column
train.drop(['light_conditions',
 'weather_conditions',
 'road_surface_conditions',
 'vehicle_type',
 'junction_location',
 'skidding_and_overturning',
 'vehicle_leaving_carriageway',
 'hit_object_off_carriageway',
 'first_point_of_impact',
 'sex_of_driver','accident_severity'],axis=1,inplace=True)

# Split the dataset into features and target labels
X = train.iloc[:, :-1].values
y = train.iloc[:, -1].values

# Apply PCA to reduce the dimensionality of the features
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Plot the reduced dataset with the target labels
plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y)
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

# correlation of the preprocessed data
fig,ax = plt.subplots(figsize=(10,8))
correlations_ML = train.corr()

sns.heatmap(correlations_ML,cmap="YlGnBu",annot=True)
plt.show()

"""#MACHINE LEARNING MODELS"""

#Selection of the variables
x = train.drop('accident_severity_cat',axis=1)
y = train['accident_severity_cat']

#Splitting the data
train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.2,random_state=101,stratify=y)

"""BASELINE MODELLING FOR ML"""

# Generate some toy data with 3 classes and 1000 samples
X = np.random.rand(1000, 10)
y = np.random.randint(0, 3, size=1000)

# Define the majority class baseline
baseline = DummyClassifier(strategy='most_frequent')

# Fit the baseline model to the training data
baseline.fit(X, y)

# Make predictions on new data
y_pred = baseline.predict(X)

# Evaluate the accuracy of the baseline model
accuracy = accuracy_score(y, y_pred)
f1 = f1_score(y, y_pred, average='weighted')
print('Accuracy of majority class baseline:', accuracy)
print('F1_score:', f1)

"""MODEL 1: RANDOM FOREST CLASSIFIER"""

#modelling
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(train_x,train_y)

#Making prediction
predict_1 = rf_model.predict(test_x)

#Measuring accuracy
print(classification_report(test_y,predict_1))
print("Accuracy:",accuracy_score(predict_1,test_y))

# compute confusion matrix
con = confusion_matrix(test_y,predict_1)
print("Confusion matrix:\n", con)

# compute F1 score
f1 = f1_score(test_y,predict_1,average='weighted')
print("F1 score:", f1)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = con, display_labels = class_nam)

cm_display.plot()
plt.show()

"""RANDOM FOREST WITH HYPERPARAMETER TUNING"""

# Define the hyperparameters to be tuned
param_grid = {'n_estimators': [100, 500, 1000],
              'max_depth': [10, 20, 30]}

# Create the Random Forest Classifier model
rfc = RandomForestClassifier(random_state=42)

# Perform grid search cross validation with 5-fold CV
grid_search = GridSearchCV(rfc, param_grid, cv=5)

# Fit the grid search to the data
grid_search.fit(X, y)

# Print the best hyperparameters found
print(grid_search.best_params_)

#modelling
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(train_x,train_y)

#Making prediction
predict_1 = rf_model.predict(test_x)

#Measuring accuracy
print(classification_report(test_y,predict_1))
print("Accuracy:",accuracy_score(predict_1,test_y))
f1 = f1_score(test_y,predict_1,average='weighted')
print("F1 score:", f1)

"""MODEL 2: DECISION TREE CLASSIFIER"""

#modelling
df_model = DecisionTreeClassifier(random_state=42)
df_model.fit(train_x,train_y)

#Making prediction
predict_2 = df_model.predict(test_x)

#Measuring accuracy
print(classification_report(test_y,predict_2))
print("Accuracy:",accuracy_score(predict_2,test_y))

# compute confusion matrix
con1 = confusion_matrix(test_y,predict_2)
print("Confusion matrix:\n", con1)

# compute F1 score
f1 = f1_score(test_y,predict_2,average='weighted')
print("F1 score:", f1)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = con1, display_labels = class_nam)

cm_display.plot()
plt.show()

"""MODEL3: LOGISTIC REGRESSION"""

#modelling
lr_model = LogisticRegression(solver='lbfgs',max_iter=100,multi_class='auto', random_state=42)
lr_model.fit(train_x,train_y)

#Making prediction
predict_3 = lr_model.predict(test_x)

#Measuring accuracy
print(classification_report(test_y,predict_3))
print("Accuracy:",accuracy_score(predict_3,test_y))

# compute confusion matrix
con2 = confusion_matrix(test_y,predict_3)
print("Confusion matrix:\n", con2)

# compute F1 score
f1 = f1_score(test_y,predict_3,average='weighted')
print("F1 score:", f1)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = con2, display_labels = class_nam)

cm_display.plot()
plt.show()

"""MODEL 4: SVC"""

#modelling
sv_model = SVC(random_state=42)
sv_model.fit(train_x,train_y)

#Making prediction
predict_4 = sv_model.predict(test_x)

#Measuring accuracy
print(classification_report(test_y,predict_4))
print("Accuracy:",accuracy_score(predict_4,test_y))

# compute confusion matrix
con3 = confusion_matrix(test_y,predict_4)
print("Confusion matrix:\n", con3)

# compute F1 score
f1 = f1_score(test_y,predict_4,average='weighted')
print("F1 score:", f1)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = con3, display_labels = class_nam)

cm_display.plot()
plt.show()

"""# NEURAL NETWORK MODELS

Baseline Modelling for Neural Network
"""

# Load the iris dataset
data = load_iris()
X, y = data.data, data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the majority class baseline classifier
dummy = DummyClassifier(strategy='most_frequent')

# Fit the classifier to the training data
dummy.fit(X_train, y_train)

# Predict the class labels for the test data
y_pred = dummy.predict(X_test)

# Evaluate the performance of the classifier using accuracy and F1-score
acc = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='weighted')

print('Majority class baseline:')
print('Accuracy: {:.3f}'.format(acc))
print('F1-score: {:.3f}'.format(f1))

"""1. Recurrent Neural Networks"""

#Selection of the variables
x = train.drop('accident_severity_cat',axis=1)
y = train['accident_severity_cat']

#train and test split
X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=103)

# Reshape the input data to fit the RNN input shape (batch_size, timesteps, features)
batch_size = 32
timesteps = 1
features = X_train.shape[1]
#X_train = X_train.reshape(-1, timesteps, features)
#X_test = X_test.reshape(-1, timesteps, features)

# Create the RNN model
model = Sequential()
model.add(Dense(units=64, activation='relu', input_dim=x.shape[1]))
model.add(Dense(units=32,activation='relu'))
model.add(Dense(units=16,activation='relu'))
model.add(Dense(units=8,activation='relu'))
model.add(Dense(units=3, activation='sigmoid'))
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
print(model.summary())

# Train the model
epochs = 10
history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,verbose=1, validation_data=(X_test, y_test))

# Evaluate the model on the testing set
test_loss, test_acc = model.evaluate(X_test, y_test,verbose=0)
print('Test accuracy:', test_acc)
print('Test loss:', test_loss)

# prediction
y_pred = model.predict(X_test)
y_pred_labels = np.argmax(y_pred, axis=1)

#compute classification report
print(classification_report(y_test,y_pred_labels))
print("Accuracy:",accuracy_score(y_test,y_pred_labels))


# compute confusion matrix
confusion = confusion_matrix(y_test,y_pred_labels)
print("Confusion matrix:\n", confusion )

# compute F1 score
f1 = f1_score(y_test,y_pred_labels,average='weighted')
print("F1 score:", f1)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion, display_labels = class_nam)

cm_display.plot()
plt.show()

"""2. Multilayer Perceptron (MLP)"""

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(train.drop('accident_severity_cat', axis=1), train['accident_severity_cat'], test_size=0.2, random_state=42)

# Create an instance of the MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(100,50), max_iter=1000)

# Train the MLPClassifier on the training data
mlp.fit(X_train, y_train)

# Use the trained MLPClassifier to predict the labels of the testing data
y_pred1 = mlp.predict(X_test)

# Evaluate the accuracy of the MLPClassifier on the testing data
accuracy = accuracy_score(y_test, y_pred1)
print('Accuracy:', accuracy)

#compute classification report
print(classification_report(y_test,y_pred1))
print("Accuracy:",accuracy_score(y_test,y_pred1))


# compute confusion matrix
confusion1 = confusion_matrix(y_test,y_pred1)
print("Confusion matrix:\n", confusion1 )

# compute F1 score
f2 = f1_score(y_test,y_pred1,average='weighted')
print("F1 score:", f2)

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion1, display_labels = class_nam)

cm_display.plot()
plt.show()

"""3. BEST HYPERPARAMETER TUNING"""

# Define the parameter grid for hyperparameter tuning
param_grid = {'hidden_layer_sizes': [(50,),(100,),(200,)],
              'activation': ['relu', 'tanh', 'logistic'],
              'alpha': [0.0001, 0.001, 0.01]}

# Create the MLP classifier
mlp = MLPClassifier(random_state=42, max_iter=1000)

# Create the grid search object
grid_search = GridSearchCV(mlp, param_grid=param_grid, cv=5, verbose=2)

# Fit the grid search object to the training data
grid_search.fit(X_train, y_train)

# Print the best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)

# Make predictions on the testing data using the best hyperparameters
y_pred = grid_search.predict(X_test)

# Calculate the accuracy score, classification report, and confusion matrix
accuracy = accuracy_score(y_test, y_pred)
f3 = f1_score(y_test,y_pred,average='weighted')
print("F1 score:", f3)
print("Accuracy:", accuracy)
print("Classification report:\n", classification_report(y_test, y_pred))
print("Confusion matrix:\n", confusion_matrix(y_test, y_pred))

# Define the class names
class_nam = ['Slight', 'Serious', 'Fatal']
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = class_nam)

cm_display.plot()
plt.show()